{"meta":{"title":"Bowen He's Blog","subtitle":"劝君惜取少年时.","description":null,"author":"Bowen He","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"一些python function总结","slug":"python_script_function","date":"2017-11-10T07:07:00.000Z","updated":"2017-11-10T07:10:02.000Z","comments":true,"path":"2017/11/09/python_script_function/","link":"","permalink":"http://yoursite.com/2017/11/09/python_script_function/","excerpt":"","text":"今天看一份代码的时候，有些python function不知其何用，想来写个博客把每次遇到的总结在一篇blog，可以时常看看。 print(__doc__)这个可以print出python file or function中注释部分，比如： in module.py: 123456&quot;&quot;&quot;This is the module file, use for provide a function.&quot;&quot;&quot;def func(x): return x&gt;&gt;&gt;import module&gt;&gt;&gt;print(module.__doc__)&gt;&gt;&gt;&apos;This is the module file, use for provide a function.&apos;","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"kaggle dog breed identification基于Tensorflow迁移学习搭建图片分类器","slug":"kaggle_dog_breed_idnentification","date":"2017-11-04T23:50:00.000Z","updated":"2017-11-11T00:40:36.000Z","comments":true,"path":"2017/11/04/kaggle_dog_breed_idnentification/","link":"","permalink":"http://yoursite.com/2017/11/04/kaggle_dog_breed_idnentification/","excerpt":"","text":"基于Tensorflow的迁移学习应用 kaggle dog breed identification构建新图片分类器本文所用数据来源于https://www.kaggle.com/c/dog-breed-identification, 将基于google tensorflow中的预训练的mobilenet模型和inception v3模型，对新dataset中10200张不同狗品种照片训练新的图片分类。 Why transfer learning here? 一个好的图片分类器，从scratch开始搭建，不仅构建cnn archtiecture会花费大量的时间精力，而且为了分类器的精准度，也需要大量的图片作为training data。 而实际应用中，针对不用需要而构建的分类器，通常只有小量的数据集，从scratch开始训练一个巨大的拥有百万甚至更多的parameters的cnn分类器很容导致过拟合，而使训练出的分类器在general set上表现不佳。 我们将使用的经过预训练的mobilenet模型和inception v3模型，是google基于 ImageNet , 一个计算机视觉系统识别项目，目前世界上图像识别最大的数据库，其包含了分成了数千个类型、数百万张有标注的图像, 训练出的两个分类器模型，其Top-1 accuracy 分别达到78.0%和70.7%，inception v3会有更好的accuray，而mobilenet因为其轻量，训练速度更快。 针对与原数据库相似但size较小的数据集，使用transfer learning，将已根据 基于MobileNet的新分类器搭建准备阶段，get code12git clone https://github.com/googlecodelabs/tensorflow-for-poets-2cd tensorflow-for-poets-2 准备阶段，get datasethttps://www.kaggle.com/c/dog-breed-identification, 下载training dataset, labels.csv和test dataset, 预处理数据集我们直接下载得到的训练数据集中，只有所有的图片在同一个数据集，我们需要整理成模型需要的格式，即在同一个directory下，将每个label建一个subdirectory，将所有对应的图片放入当前subdirectory下，参考code可见: https://github.com/LauraBowenHe/dog-breed-identification/blob/master/MobileNet/classify_pic_to_dir.py 配置MobileNet12IMAGE_SIZE=224ARCHITECTURE=&quot;mobilenet_1.0_$&#123;IMAGE_SIZE&#125;&quot; 注意这里： IMAGE_SIZE, 即image resolution配置的参数：128,160,192, or 224px。当然越高的resolution可以带来更高的accuray，同时也需要更多的训练时间。 相对模型size占最大largest MobileNet的比例: 1.0, 0.75, 0.50, or 0.25. 同样，所占比例越大，accuracy越高，也需要更多的训练时间。 在这个任务中，我们希望获得更可能高的accuray，所以采取以上配置。 用tensorboard监控训练过程1tensorboard --logdir tf_files/training_summaries &amp; 在后台启用tensorboard，tensorboard可以用来监控训练过程，帮助我们更好判断是否过拟合等。 训练完成后，可以在浏览器输入 http://localhost:6006 or http://0.0.0.0:6006即可看到tensorboard为我们绘画的各种训练过程图像。 训练模型12345678910python -m scripts.retrain \\ --bottleneck_dir=tf_files/bottlenecks \\ --how_many_training_steps=3000 \\ --model_dir=tf_files/models/ \\ --summaries_dir=tf_files/training_summaries/&quot;$&#123;ARCHITECTURE&#125;&quot; \\ --output_graph=tf_files/retrained_graph.pb \\ --output_labels=tf_files/retrained_labels.txt \\ --architecture=&quot;$&#123;ARCHITECTURE&#125;&quot; \\ --image_dir=tf_files/dog_photos 注意： 在这里，把我们之前预处理过的数据集，让在tf_files directory下。 optionally，我们可以添加更多的训练参数，详情可以参考 1234python -m scripts.retrain -h我加入了以下的参数--training_rate=0.001 \\--train_batch_size=64 \\ 调参调节training steps， training rate， train batch size等等。 预测对于单个图片，可以用source code中的label_image.py 123python -m scripts.label_image \\ --graph=tf_files/retrained_graph.pb \\ --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg 而针对这个狗品种分类器的任务，我们把从kaggle上下载的test dataset也放入tf_files directory下，修改了一下source中的label_image.py，增加新的文件generate_result.py, 对每个文件依次调用label_image.py，code可见 https://github.com/LauraBowenHe/dog-breed-identification/blob/master/MobileNet/scripts/label_image.py https://github.com/LauraBowenHe/dog-breed-identification/tree/master/MobileNet 1python generate_result.py 这一步预测时间会比较长，大概7-8小时，生成的文件会存在/tensorflow-poets-2/output/下，我们在训练前可先手动创建这个文件夹。 基于inception v3的新分类器搭建inception v3的搭建和mobilenet很相似，只有在get code时有所不同。 准备阶段，get code123$ git clone https://github.com/tensorflow/tensorflow$ cd tensorflow$ git checkout 准备阶段, get dataset和预处理数据集参考以上 训练模型retrain的代码在/examples/image_retraining，我们可以直接用刚刚mobilenet模型中，/tensorflow-for-poet-2/tf_files，因为我们之前已经有把数据集都放在那里了，当然即使没有也没关系，新建一个tf_files就行 12345678python retrain.py \\ --bottleneck_dir=~/tensorflow-for-poets-2/tf_files/bottlenecks \\ --how_many_training_steps=500 \\ --model_dir=~/tensorflow-for-poets-2/tf_files/inception \\ --summaries_dir=~/tensorflow-for-poets-2/tf_files/training_summaries/basic \\ --output_graph=~/tensorflow-for-poets-2/tf_files/retrained_graph.pb \\ --output_labels=~/tensorflow-for-poets-2/tf_files/retrained_labels.txt \\ --image_dir=~/tensorflow-for-poets-2/tf_files/dog_photos python -m retrain.py -h 可以查阅可以添加的参数 附一张训练结束的图，可见当前分类器的validation accuracy是90.3%。 调参调节training steps， training rate， train batch size等等。 预测因为我们已将生成的模型放入tf_files。我们把从kaggle上下载的test dataset也放入tf_files directory下，创建了一份label_image_inceptionV3.py，增加新的文件generate_result_inceptionV3.py, 对每个文件依次调用label_image_inceptionV3.py，code可见 https://github.com/LauraBowenHe/dog-breed-identification/blob/master/InceptionV3/label_image_inceptionV3.py https://github.com/LauraBowenHe/dog-breed-identification/blob/master/InceptionV3/generate_result_inceptionV3.py 1python generate_result_inceptionV3.py 这一步预测时间会比较长，比MobileNet所用的时间更长，大概在十四个小时，生成的文件会存在/tensorflow-poets-2/output/下，我们在训练前可先手动创建这个文件夹。 Referencehttp://cs231n.github.io/transfer-learning/ https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0 https://www.ouyangsong.com/2017/05/20/image-classification-retrained-based-on-inceptionv3/","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"爬虫笔记4","slug":"一些在爬虫中用到的python tricks","date":"2017-06-26T23:50:00.000Z","updated":"2017-11-05T00:08:06.000Z","comments":true,"path":"2017/06/26/一些在爬虫中用到的python tricks/","link":"","permalink":"http://yoursite.com/2017/06/26/一些在爬虫中用到的python tricks/","excerpt":"","text":"一些在爬虫中用到的python tricksstring.strip([c]) //This method returns a copy of the string in which all chars have been stripped from the beginning and the end of the string. 返回一个string，是原string的copy但删除了所有的c字符。 Reference: https://www.tutorialspoint.com/python/string_strip.htm .strip() removes all whitespace at the start and end, including spaces, tabs, newlines and carriage returns. Reference: https://stackoverflow.com/questions/13013734/string-strip-in-python 1str.split(str=&quot;&quot;, num=number) //str是分隔符，默认是space 举例说明吧， 123str = &quot;Line1-abcdef \\nLine2-abc \\nLine4-abcd&quot;;print str.split( )print str.split(&apos; &apos;, 1 ) 结果： 12[&apos;Line1-abcdef&apos;, &apos;Line2-abc&apos;, &apos;Line4-abcd&apos;][&apos;Line1-abcdef&apos;, &apos;\\nLine2-abc \\nLine4-abcd&apos;] Reference: https://www.tutorialspoint.com/python/string_split.htm Unicode A character is not, not, not a byte. a character is the platonic ideal of the smallest unit of textA. character encoding defines a mapping between our platonic characters and some way of representing them as bytes. Because there are many ways of representing the same character as bytes, this means that if you have a series of bytes, but do not know their encoding - even if you know the data is textual - the data is meaningless. First thing before everything, is knowing the encoding. In python, there are three distict string types: ‘unicode’, which represents unicode strings (text strings). ‘str’, which represents byte strings (binary data). ‘basestring’, which acts as a parent class for both of the other string types. Conversion between the two types is meaningless without an encoding, Python relies on a ‘default encoding’, specified by sys.setdefaultencoding(). Simply set encoding with function sys.setdefaultencoding() is a solution but may not that good, since the web may use multiple different text encoding. Here is a correction solution, referenced from: http://blog.notdot.net/2010/07/Getting-unicode-right-in-Python All text strings, everywhere should be of type unicode, not str. If you’re handling text, and your variable is a str, it’s a bug! To decode a byte string as text, use var.decode(encoding) (eg, var.decode(&#39;utf-8&#39;), with the correct encoding. To encode a text string as bytes, use var.encode(encoding). Never ever use str() on a unicode string, or unicode() on a byte string without a second argument specifying the encoding. Whenever you read data from outside your app, expect it to be bytes - eg, of type str - and call .decode() on it to interpret it as text. Likewise, always call .encode() on text you want to send to the outside world. If a string literal in your code is intended to represent text, it should always be prefixed with ‘u’. In fact, you probably never want to define a raw string literal in your code at all. For what it’s worth, though, I’m terrible at this one, as I’m sure pretty much everyone else is, too. Usually in python 2, for a web crawler python file, I see a lot: 12reload(sys)sys.setdefaulyencoding(&apos;utf8&apos;) But in python3, default is UTF-8 already. No point to write this again.","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"爬虫笔记3","slug":"Avoid Scraper Trapper","date":"2017-06-25T23:45:57.000Z","updated":"2017-07-03T07:26:37.000Z","comments":true,"path":"2017/06/25/Avoid Scraper Trapper/","link":"","permalink":"http://yoursite.com/2017/06/25/Avoid Scraper Trapper/","excerpt":"","text":"Avoid Scraper Trapperrequests library enables use to handle form on website, is algo good at setting headers. HTTP headers contains attributes, preferences, sent by you every time you make a request to server. Header used by a typical Python scraper using the default url lib library might send: Accept-Encoding: identity User-Agent: Python-urllib/3.4 Good website, https://www.whatismybrowser.com , to test browser properties viewable by server. Usually the one setting, that really matters for websites to check for “humanness” based on, is “User-Agent”. Headers change bring a lot conveincelet’s say you need some Chinese material, just simply changing Accept-language: en-US to Accept-Language: zh. Mobile devices have a different version of web page, so set as this: User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X AppleWebKi/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257) brings a great change. Handling CookiesCookies can keep you logged in on a site. There are a number of browser plug-ins that can show you how cookies are being set as you visit and move, https://www.editthiscookie.com/ , a Chrome extension, is very good. Request library will be unsable to handle many of the cookies produced by modern software, use Selenium and PhantomJS packages. 12345678910111213141516171819driver = web driver.PhantomJS(executable_path = &apos;&lt;Path to Phantom JS&gt;&apos;)driver.get(&apos;https://pythonscraping.com&apos;)driver.implicitly_wait(1)driver.get_cookies() //save cookiesdriver2.delete_all_cookies() //delete all cookiesfor cookie in savedCookies:driver2.add_cookies(cookie)driver2.get(&quot;http://pythonscraping.com&quot;)driver.implicitly_wait(1)print(driver2.get_cookies()) Timing Is EveythingEven sometimes use multithreaded jobs can make your scraper faster than one single thread, but keep individual page loads and data requests to a minimum, can try to space them by a few seconds, time.sleep(3). Reference: Book: Web Scraping with Python: Collecting Data from the Modern Web. https://www.pythonscraping.com/","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"爬虫笔记2","slug":"handling redirects","date":"2017-06-24T23:40:55.000Z","updated":"2017-07-03T07:32:01.000Z","comments":true,"path":"2017/06/24/handling redirects/","link":"","permalink":"http://yoursite.com/2017/06/24/handling redirects/","excerpt":"","text":"handling redirectsserver-side redirect, depending on how it is handled, can be easily traversed by Python’s urllib library without any help from Selenium; client-side redirects won’t be handled at all unless something is actually executing the javascript. selenium is capable of handling these Javascript redirects in the same way; when to stop page execution? how to tell when a page is done rediecting? Detect that redirected in a clever way by watcing an element in the DOM when the page initially loads, then repeatedly calling the element until Selenium throws a StaleElementReferenceException, the element is no longer attached to the page’s DOM and the site has redirected. Image Processing and Text RecognitionPillowPillow allows you to easily import and manipulate images iwth a variety of filters, masks, and even pixel-specifc transformations. from PIL import import Image, ImageFilter kitten = Image.open(“kitten.jpg”) blurryKitten = kitten.filter(ImageFilter.GaussianBlur) blurryKitten.save(“kitten_blurred.jpg”) blurrykitten.show() for more useful, http://pillow.readthedocs.org/ Tesseractscrape text from images on webste.","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"leetcode/lintcode刷题系－Expression Expand","slug":"Expression Expand","date":"2017-06-23T03:18:00.000Z","updated":"2017-07-03T07:32:17.000Z","comments":true,"path":"2017/06/22/Expression Expand/","link":"","permalink":"http://yoursite.com/2017/06/22/Expression Expand/","excerpt":"","text":"Expression ExpandGiven an expression s includes numbers, letters and brackets. Number represents the number of repetitions inside the brackets(can be a string or another expression)．Please expand expression to be a string. Examples = abc3[a] return abcaaas = 3[abc] return abcabcabcs = 4[ac]dy, return acacacacdys = 3[2[ad]3[pf]]xyz, return adadpfpfpfadadpfpfpfadadpfpfpfxyz 语法整理： Integer.valueOf(int i)//returns an integer object holds the value of specified primitives, 返回一个保存原指定数的整数对象。 Integer.valueOf(String s)//This returns an Integer object holding the value of the specified string representation.返回一个保存原指定string的整数对象。 Stack stack = new Stack(); //a templetate of stack, use explicit conversion to convert to some class. Character.isDigit(c) //check if a character is ‘0’-‘9’ Integer count = (Integer)stack.pop(); instanceof //check type of the object public class Solution { /** * @param s an expression includes numbers, letters and brackets * @return a string */ public String expressionExpand(String s) { // Write your code here Stack&lt;Object&gt; stack = new Stack&lt;Object&gt;(); int number = 0; for(char c: s.toCharArray()){ if(Character.isDigit(c)){ number = number * 10 + (c-&apos;0&apos;); }else if(c == &apos;[&apos;){ stack.push(Integer.valueOf(number)); number = 0; }else if(c == &apos;]&apos;){ String temp = popStack(stack); Integer count = (Integer)stack.pop(); while(count &gt; 0){ stack.push(temp); count--; } }else{ stack.push(String.valueOf(c)); } } return popStack(stack); } String popStack(Stack&lt;Object&gt; stack){ Stack&lt;String&gt; container = new Stack&lt;String&gt;(); while(!stack.empty() &amp;&amp; (stack.peek() instanceof String)){ container.push((String)stack.pop()); } StringBuilder sb = new StringBuilder(); while(!container.empty()){ sb.append(container.pop()); } return sb.toString(); } }","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"leetcode/lintcode刷题系－Sliding Window Median","slug":"Sliding Window Median","date":"2017-06-22T22:51:00.000Z","updated":"2017-06-23T03:18:19.000Z","comments":true,"path":"2017/06/22/Sliding Window Median/","link":"","permalink":"http://yoursite.com/2017/06/22/Sliding Window Median/","excerpt":"","text":"Sliding Window MedianQuestion: Given an array of n integer, and a moving window(size k), move the window at each iteration from the start of the array, find the median of the element inside the window at each moving. (If there are even numbers in the array, return the N/2-th number after sorting the element in the window. ) ExampleFor array [1,2,7,8,5], moving window size k = 3. return [2,7,7]At first the window is at the start of the array like this[ | 1,2,7 | ,8,5] , return the median 2;then the window move one step forward.[1, | 2,7,8 | ,5], return the median 7;then the window move one step forward again.[1,2, | 7,8,5 | ], return the median 7; 分析： 首先，这道题目是一道移动窗口求中位数类的问题，对于动态求中位数的问题，通常我会用heap的模版来处理，即维持两个heap，minheap和maxheap，maxheap来存放所有比中位数小的及相等的数＋中位数，minheap存放所有比中位数大的数；举例说明，当我们的数组是［1，2，7］，这个时候maxheap存放［1，2］，minheap存放［7］，中位数就是maxheap里的最大值，当有新的数进来或者旧的数出去的时候，我们每次都和当前median的大小来比较，决定放入哪个heap，再根据左右两边size的大小进行相应调整，永远满足median是 N/2-th number。 选用什么数据结构？因为这里的动态窗口还要求删除被移除窗口的元素，所以我们需要一个数据结构，既能满足heap的性质，又能满足定点删除的性质，于是想到了hashheap！java中TreeSet正好满足。 这里有一个tricky的点在于，因为input中的输入点可能是duplicated，直接把数放到treeset中，不能存下duplicated elements，怎么解决？想到了可以构造一个class，同时包含这个element在input array的位置，即id，还包含这个element的value，那么我们将得到unique的object。 代码： public class Solution { /** * @param nums: A list of integers. * @return: The median of the element inside the window at each moving. */ public Node insertNum(Node median, Node currNode, TreeSet&lt;Node&gt; minheap, TreeSet&lt;Node&gt; maxheap){ /* median is put on the maxheap */ if(maxheap.size() == 0 &amp;&amp; minheap.size() == 0){ maxheap.add(currNode); return currNode; } if(currNode.val &lt;= median.val){ if(maxheap.size()-minheap.size() &lt; 1){ maxheap.add(currNode); median = maxheap.last(); }else{ minheap.add(median); maxheap.remove(median); maxheap.add(currNode); median = maxheap.last(); } }else{ if(maxheap.size() &gt; minheap.size() ){ minheap.add(currNode); }else{ minheap.add(currNode); median = minheap.first(); maxheap.add(median); minheap.remove(median); } } return median; } public ArrayList&lt;Integer&gt; medianSlidingWindow(int[] nums, int k) { // write your code here ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(nums.length == 0 || k &lt;= 0 || nums.length &lt; k){ return list; } TreeSet&lt;Node&gt; minheap = new TreeSet&lt;Node&gt;(); TreeSet&lt;Node&gt; maxheap = new TreeSet&lt;Node&gt;(); Node median = new Node(0, nums[0]); maxheap.add(median); for(int i = 1; i &lt; k; i++){ Node currNode = new Node(i, nums[i]); median = insertNum(median, currNode, minheap, maxheap); } list.add(median.val); for(int i = k; i &lt; nums.length; i++){ Node currNode = new Node(i, nums[i]); Node prevNode = new Node(i-k, nums[i-k]); if (minheap.contains(prevNode)) { minheap.remove(prevNode); } else{ maxheap.remove(prevNode); } median = insertNum(median, currNode, minheap, maxheap); list.add(median.val); } return list; } } /* construct node class */ class Node implements Comparable&lt;Node&gt;{ int id; int val; Node(int id, int val){ this.id = id; this.val = val; } /* construct compareTo method, to make sure the elements is sorted by val or ID when vals are same */ public int compareTo(Node other){ if(this.val == other.val){ return this.id-other.id; } return this.val-other.val; } }","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"爬虫中关于登录网页，文件上传的一些笔记","slug":"爬虫中遇到的登录login和form","date":"2017-06-22T06:21:57.000Z","updated":"2017-07-03T07:31:10.000Z","comments":true,"path":"2017/06/21/爬虫中遇到的登录login和form/","link":"","permalink":"http://yoursite.com/2017/06/21/爬虫中遇到的登录login和form/","excerpt":"","text":"爬虫中遇到的登录login和form大多数web form都是由一些HTML field, submit button, 以及一个”action” page，即form真正被处理的地方，共同组成。举例说明，以下为一个最基本的web form:http://pythonscraping.com/pages/files/form.html, 可以用chrome浏览器，view-developer-source code查看html代码。 通过查看代码，要注意两点： [ ] 两个input field的名字是firstname和last name， 这两个名字是我们一会儿要POST到server去的。 [ ] 这里处理form的文件是processing.php，也是我们一会要传送的。 怎么在爬虫中完成登录，在这里介绍python的Request library，非常简单的代码： `import requests params = {‘firstname’: ‘Ryan’, ‘lastname’: ‘Mitchell’}r = requests.post(“http://pythonscraping.com/files/processing.php“, data = params)print(r.text)` 当然，这里的例子用的form很简单，但是当面对登录页面复杂的HTML代码时，我们也不用过于紧张，我们只需要寻找两类： [ ] 想要提交的数据的field的名字, 比如上面例子的firstname和lastname。 [ ] 真正处理form的action page。 爬虫中遇到的Radio Buttons, Checkboxes和其他输入标准HTML文件包含了很多可能的form input fields，比如radio buttons, checkboxes, select boxes, 在 HTML5中，还有emails, dates等等，尽管登录页面代码看着很复杂，但我们要集中查找以上谈过的两类： [ ] 想要提交的数据的field的名字, 比如上面例子的firstname和lastname。 [ ] 真正处理form的action page。 有一种非常便捷的方法查看GET REQUESTS， 就是查看这个site的url，举例说明：假设url长这样http: //domainname.com?thing1=foo&amp;thing2=bar。 上传文件和图片举例说明操作吧，https://www.pythonscraping.com/files/form2.html， 查看其source code后，我们发现和第一个例子的source code很像，不同点在于这里的 type是file，代码如下： `import requests files = {‘uploadFile’: open(‘.,/files/pic.png’, ‘rb’)}r =requests(“https://www.pythonscraping.com/pages/processing2.php“, files=files)print(r.text)`","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"leetcode/lintcode刷题系－Trapping Rain Water 类问题的分析","slug":"leetcode:lintcode刷题系－Trapping Rain Water 类问题的分析","date":"2017-06-21T21:20:00.000Z","updated":"2017-06-22T22:50:42.000Z","comments":true,"path":"2017/06/21/leetcode:lintcode刷题系－Trapping Rain Water 类问题的分析/","link":"","permalink":"http://yoursite.com/2017/06/21/leetcode:lintcode刷题系－Trapping Rain Water 类问题的分析/","excerpt":"","text":"Trapping Rain Water: Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. Example: Given [0,1,0,2,1,0,1,3,2,1,2,1], return 6. 对于这道灌水题，因为水只有在凹槽中才能储存，因此可以用左右两个指针，一个指着array最左端，一个指着array最右端，分别标记此时左右两端的高度lheight, rheight, 取两个值中的较小值，假设是lheight，这个高度便是我们的“灌水基调”，使这个bar的指针往中间移动，，指向下一个bar，当下一个bar高度小于当前“灌水基调”，那么这个位置是可以储存住水的，两者高度差就是灌水量，而若下一个bar高度大于当前”灌水基调“，那么我们更新lheight为当前bar的高度，进入下一次循环，划线部分重复，直到左右指针相交。 ` public class Solution { /** * @param heights: an array of integers * @return: a integer */ public int trapRainWater(int[] heights) { // write your code here /* check if input is valid */ int left = 0, right = heights.length-1, res = 0; if(heights == null || heights.length == 0){ return res; } int leftHeight = heights[left]; int rightHeight = heights[right]; while(left &lt; right){ if(leftHeight &lt; rightHeight){ left++; if(heights[left] &lt; leftHeight){ res += leftHeight-heights[left]; }else{ leftHeight = heights[left]; } }else{ right--; if(heights[right] &lt; rightHeight){ res += rightHeight-heights[right]; }else{ rightHeight = heights[right]; } } } return res; } `","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"Hello!","slug":"first-one","date":"2017-06-12T22:52:57.000Z","updated":"2017-11-04T23:55:27.000Z","comments":true,"path":"2017/06/12/first-one/","link":"","permalink":"http://yoursite.com/2017/06/12/first-one/","excerpt":"","text":"开博客啦！","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]}]}